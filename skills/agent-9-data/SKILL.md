---
name: agent-9-data
version: 1.0.0
description: æ•°æ®å¤„ç†åˆ†æå¸ˆAgentæŠ€èƒ½ - è´Ÿè´£æ•°æ®æ¨¡å‹è®¾è®¡ã€æ•°æ®åº“è¿ç§»ã€æµ‹è¯•æ•°æ®ç”Ÿæˆå’Œæ•°æ®è´¨é‡åˆ†æã€‚ä½¿ç”¨PostgreSQL + MongoDB + Redis + Flyway + FakeræŠ€æœ¯æ ˆã€‚
author: Digital Bank POC Team
license: MIT
keywords:
  - database
  - postgresql
  - mongodb
  - data-modeling
  - data-migration
  - test-data
  - data-quality
  - er-diagram
---

# Agent 9: æ•°æ®å¤„ç†åˆ†æå¸ˆ ğŸ“Š

## æ¦‚è¿°

Agent 9è´Ÿè´£æ•°æ®æ¨¡å‹è®¾è®¡ã€æ•°æ®åº“è¿ç§»ã€æµ‹è¯•æ•°æ®ç”Ÿæˆå’Œæ•°æ®è´¨é‡åˆ†æã€‚ç¡®ä¿æ•°æ®æ¨¡å‹è®¾è®¡åˆç†ï¼Œæµ‹è¯•æ•°æ®å……è¶³ï¼Œæ•°æ®è´¨é‡è¾¾æ ‡ã€‚

## ä½•æ—¶ä½¿ç”¨

å½“éœ€è¦ï¼š
- è®¾è®¡æ•°æ®æ¨¡å‹å’ŒERå›¾
- åˆ›å»ºæ•°æ®åº“Schema
- ç”Ÿæˆæµ‹è¯•æ•°æ®
- æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥
- åˆ†ææ•°æ®æ€§èƒ½

## æŠ€æœ¯æ ˆ

- **å…³ç³»æ•°æ®åº“**: PostgreSQL 15
- **æ–‡æ¡£æ•°æ®åº“**: MongoDB 7.0
- **ç¼“å­˜**: Redis
- **æ•°æ®è¿ç§»**: Flyway
- **æ•°æ®ç”Ÿæˆ**: Python Faker, Faker.js
- **æ•°æ®åˆ†æ**: Pandas
- **æ•°æ®è´¨é‡**: Great Expectationsï¼ˆå¯é€‰ï¼‰

## æ ¸å¿ƒåŠŸèƒ½

### 1. æ•°æ®æ¨¡å‹è®¾è®¡
- ERå›¾è®¾è®¡
- PostgreSQL Schemaè®¾è®¡
- MongoDB Collectionè®¾è®¡
- æ•°æ®å­—å…¸ç”Ÿæˆ

### 2. æ•°æ®åº“è¿ç§»
- Flywayè¿ç§»è„šæœ¬
- Schemaç‰ˆæœ¬ç®¡ç†
- æ•°æ®è¿ç§»è„šæœ¬

### 3. æµ‹è¯•æ•°æ®ç”Ÿæˆ
- è´¦æˆ·æ•°æ®ï¼š10ä¸‡+æ¡
- å®¢æˆ·æ•°æ®ï¼š5ä¸‡+æ¡
- äº¤æ˜“æ•°æ®ï¼š100ä¸‡+æ¡
- æ•°æ®è´¨é‡éªŒè¯

### 4. æ•°æ®è´¨é‡åˆ†æ
- æ•°æ®è´¨é‡æ£€æŸ¥
- æ€§èƒ½åˆ†æ
- æ•°æ®è´¨é‡æŠ¥å‘Š

## è‡ªåŠ¨åŒ–èƒ½åŠ›

- **æ•°æ®è‡ªåŠ¨åŒ–**: 85%è‡ªåŠ¨åŒ–
  - æ•°æ®æ¨¡å‹è‡ªåŠ¨è®¾è®¡ï¼ˆåŸºäºERå›¾ï¼‰
  - Schemaè‡ªåŠ¨è¿ç§»ï¼ˆFlywayï¼‰
  - æµ‹è¯•æ•°æ®è‡ªåŠ¨ç”Ÿæˆï¼ˆFakerï¼‰
  - æ•°æ®è´¨é‡è‡ªåŠ¨æ£€æŸ¥
  - æ€§èƒ½åˆ†æè‡ªåŠ¨æ‰§è¡Œ
  - ERå›¾è‡ªåŠ¨ç”Ÿæˆ
  - æ•°æ®å­—å…¸è‡ªåŠ¨è¾“å‡º

## æ•°æ®è§„æ¨¡

- **è´¦æˆ·æ•°æ®**: 10ä¸‡+æ¡
- **äº¤æ˜“æ•°æ®**: 100ä¸‡+æ¡
- **ç”¨æˆ·æ•°æ®**: 5ä¸‡+æ¡

## äº¤ä»˜æ ‡å‡†

- **æ•°æ®æ¨¡å‹æ–‡æ¡£**: å®Œæ•´
- **æµ‹è¯•æ•°æ®**: å……è¶³å¯ç”¨
- **æ•°æ®è´¨é‡æŠ¥å‘Š**: â‰¥95åˆ†
- **æ€§èƒ½åˆ†ææŠ¥å‘Š**: è¯¦å®

## é¡¹ç›®ç»“æ„

```
database/
â”œâ”€â”€ postgresql/
â”‚   â””â”€â”€ migrations/      # Flywayè¿ç§»è„šæœ¬
â”‚       â”œâ”€â”€ V1__init_schema.sql
â”‚       â””â”€â”€ V2__add_indexes.sql
â”œâ”€â”€ mongodb/
â”‚   â””â”€â”€ schemas/         # MongoDB Schemaå®šä¹‰
â”‚       â”œâ”€â”€ payment.json
â”‚       â””â”€â”€ settlement.json
â”œâ”€â”€ test-data/           # æµ‹è¯•æ•°æ®
â”‚   â”œâ”€â”€ accounts.csv     # 10ä¸‡+è´¦æˆ·
â”‚   â”œâ”€â”€ customers.csv   # 5ä¸‡+å®¢æˆ·
â”‚   â””â”€â”€ transactions.csv # 100ä¸‡+äº¤æ˜“
â””â”€â”€ scripts/             # æ•°æ®ç”Ÿæˆè„šæœ¬
    â”œâ”€â”€ generate_accounts.py
    â”œâ”€â”€ generate_customers.py
    â””â”€â”€ generate_transactions.py

docs/
â”œâ”€â”€ data-model/
â”‚   â”œâ”€â”€ er-diagram.drawio  # ERå›¾
â”‚   â””â”€â”€ data-dictionary-v1.0.md  # æ•°æ®å­—å…¸
â””â”€â”€ data-quality-report.md  # æ•°æ®è´¨é‡æŠ¥å‘Š
```

## æŠ€æœ¯æ ‡å‡†è§„èŒƒè¦æ±‚

**é‡è¦**: å¿…é¡»ä¸¥æ ¼éµå¾ªæŠ€æœ¯æ ‡å‡†è§„èŒƒå’Œå‘½åè§„èŒƒã€‚

### å¿…é¡»éµå¾ªçš„è§„èŒƒæ–‡æ¡£

1. **æŠ€æœ¯æ ‡å‡†è§„èŒƒ v1.0**: `docs/architecture/technical-standards-v1.0.md`
2. **å‘½åè§„èŒƒ v1.0**: `docs/architecture/naming-conventions.md`

## æ•°æ®æ¨¡å‹è®¾è®¡åŸåˆ™

- éµå¾ªæ•°æ®åº“èŒƒå¼ï¼ˆè‡³å°‘3NFï¼‰
- **ä¸¥æ ¼éµå¾ªæŠ€æœ¯æ ‡å‡†è§„èŒƒä¸­çš„æ•°æ®åº“è®¾è®¡è§„èŒƒ**
- **ä¸¥æ ¼éµå¾ªå‘½åè§„èŒƒ**
- åˆç†ä½¿ç”¨ç´¢å¼•
- è€ƒè™‘æŸ¥è¯¢æ€§èƒ½
- æ•°æ®å®Œæ•´æ€§çº¦æŸ
- æ”¯æŒæ°´å¹³æ‰©å±•ï¼ˆå¦‚éœ€è¦ï¼‰

### å‘½åè§„èŒƒè¦ç‚¹

- **PostgreSQLè¡¨**: snake_case, å¤æ•° (å¦‚ `bank_accounts`, `transactions`)
- **PostgreSQLåˆ—**: snake_case (å¦‚ `account_number`, `customer_id`)
- **MongoDBé›†åˆ**: snake_case, å¤æ•° (å¦‚ `payments`, `settlements`)
- **MongoDBå­—æ®µ**: camelCase (å¦‚ `paymentId`, `orderId`, `createdAt`)
- **Elasticsearchç´¢å¼•**: snake_case (å¦‚ `risk_events`)
- **è¿ç§»è„šæœ¬**: `V{version}__{description}.sql` (å¦‚ `V1__init_schema.sql`)

## æµ‹è¯•æ•°æ®ç”Ÿæˆè¦æ±‚

- æ•°æ®çœŸå®æ€§ï¼šä½¿ç”¨Fakerç”ŸæˆçœŸå®æ„Ÿæ•°æ®
- æ•°æ®å…³è”æ€§ï¼šä¿è¯å¤–é”®å…³è”æ­£ç¡®
- æ•°æ®åˆ†å¸ƒï¼šç¬¦åˆä¸šåŠ¡åœºæ™¯åˆ†å¸ƒ
- æ•°æ®é‡ï¼šæ»¡è¶³æ€§èƒ½æµ‹è¯•éœ€æ±‚

## åä½œå…³ç³»

- **ä¸Agent 1**: æä¾›PostgreSQLæ•°æ®æ¨¡å‹
- **ä¸Agent 2**: æä¾›MongoDBæ•°æ®æ¨¡å‹
- **ä¸Agent 6**: æä¾›æµ‹è¯•æ•°æ®
- **ä¸Agent 0**: æŠ¥å‘Šæ•°æ®æ¨¡å‹è®¾è®¡

## å…³é”®é‡Œç¨‹ç¢‘

- **Day 1**: æ•°æ®æ¨¡å‹è®¾è®¡å’ŒERå›¾å®Œæˆ
- **Day 2**: æ•°æ®å­—å…¸v1.0å®Œæˆ
- **Day 3**: æ•°æ®åº“Schemaè¿ç§»å®Œæˆï¼Œæµ‹è¯•æ•°æ®ç”Ÿæˆï¼ˆè´¦æˆ·ã€å®¢æˆ·ï¼‰
- **Day 11**: å®Œæ•´æµ‹è¯•æ•°æ®ç”Ÿæˆï¼ˆ100ä¸‡+äº¤æ˜“ï¼‰ï¼Œæ•°æ®è´¨é‡æŠ¥å‘Šå®Œæˆ

## ç¤ºä¾‹ä»£ç 

### Flywayè¿ç§»è„šæœ¬ç¤ºä¾‹
```sql
-- éµå¾ªå‘½åè§„èŒƒ: V{version}__{description}.sql
-- V1__init_schema.sql

-- éµå¾ªå‘½åè§„èŒƒ: è¡¨åsnake_case, å¤æ•°å½¢å¼
CREATE TABLE bank_accounts (
    id BIGSERIAL PRIMARY KEY,  -- éµå¾ªå‘½åè§„èŒƒ: ä¸»é”®ä½¿ç”¨id
    account_number VARCHAR(20) UNIQUE NOT NULL,  -- éµå¾ªå‘½åè§„èŒƒ: åˆ—åsnake_case
    balance DECIMAL(19, 2) NOT NULL DEFAULT 0,  -- éµå¾ªå‘½åè§„èŒƒ: é‡‘é¢ä½¿ç”¨DECIMAL(19,2)
    customer_id BIGINT,  -- éµå¾ªå‘½åè§„èŒƒ: å¤–é”®ä½¿ç”¨{table}_idæ ¼å¼
    status VARCHAR(20) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,  -- éµå¾ªå‘½åè§„èŒƒ: æ—¶é—´æˆ³ä½¿ç”¨TIMESTAMP WITH TIME ZONE
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- éµå¾ªå‘½åè§„èŒƒ: ç´¢å¼•å‘½åidx_{table}_{column}
CREATE INDEX idx_bank_accounts_account_number ON bank_accounts(account_number);
CREATE INDEX idx_bank_accounts_customer_id ON bank_accounts(customer_id);
```

### æµ‹è¯•æ•°æ®ç”Ÿæˆè„šæœ¬ç¤ºä¾‹
```python
from faker import Faker
import csv

fake = Faker('zh_CN')

def generate_accounts(count=100000):
    with open('accounts.csv', 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['id', 'account_number', 'balance', 'status'])
        
        for i in range(1, count + 1):
            writer.writerow([
                i,
                fake.numerify('##########'),
                round(fake.pydecimal(left_digits=8, right_digits=2, positive=True), 2),
                fake.random_element(elements=('active', 'inactive', 'frozen'))
            ])

if __name__ == '__main__':
    generate_accounts(100000)
```

## æ•°æ®å¤‡ä»½æ¢å¤è„šæœ¬

### PostgreSQLå¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# scripts/backup/pg_backup.sh
# PostgreSQLæ•°æ®åº“å¤‡ä»½è„šæœ¬

set -e

# é…ç½®
DB_HOST="${DB_HOST:-localhost}"
DB_PORT="${DB_PORT:-5432}"
DB_NAME="${DB_NAME:-digitalbank}"
DB_USER="${DB_USER:-postgres}"
BACKUP_DIR="${BACKUP_DIR:-/backups/postgresql}"
RETENTION_DAYS="${RETENTION_DAYS:-30}"

# åˆ›å»ºå¤‡ä»½ç›®å½•
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="${BACKUP_DIR}/${DB_NAME}_${DATE}.sql.gz"
mkdir -p ${BACKUP_DIR}

echo "Starting PostgreSQL backup..."
echo "Database: ${DB_NAME}"
echo "Backup file: ${BACKUP_FILE}"

# æ‰§è¡Œå¤‡ä»½
PGPASSWORD="${DB_PASSWORD}" pg_dump \
  -h ${DB_HOST} \
  -p ${DB_PORT} \
  -U ${DB_USER} \
  -d ${DB_NAME} \
  -F c \
  -b \
  -v \
  | gzip > ${BACKUP_FILE}

# éªŒè¯å¤‡ä»½
if [ -f "${BACKUP_FILE}" ] && [ -s "${BACKUP_FILE}" ]; then
  echo "Backup completed successfully: ${BACKUP_FILE}"
  echo "Size: $(du -h ${BACKUP_FILE} | cut -f1)"
else
  echo "Backup failed!"
  exit 1
fi

# æ¸…ç†è¿‡æœŸå¤‡ä»½
echo "Cleaning up backups older than ${RETENTION_DAYS} days..."
find ${BACKUP_DIR} -name "*.sql.gz" -type f -mtime +${RETENTION_DAYS} -delete

echo "Backup process completed."
```

### PostgreSQLæ¢å¤è„šæœ¬

```bash
#!/bin/bash
# scripts/backup/pg_restore.sh
# PostgreSQLæ•°æ®åº“æ¢å¤è„šæœ¬

set -e

BACKUP_FILE=$1

if [ -z "$BACKUP_FILE" ]; then
  echo "Usage: $0 <backup_file>"
  echo "Example: $0 /backups/postgresql/digitalbank_20260126_100000.sql.gz"
  exit 1
fi

if [ ! -f "$BACKUP_FILE" ]; then
  echo "Error: Backup file not found: $BACKUP_FILE"
  exit 1
fi

# é…ç½®
DB_HOST="${DB_HOST:-localhost}"
DB_PORT="${DB_PORT:-5432}"
DB_NAME="${DB_NAME:-digitalbank}"
DB_USER="${DB_USER:-postgres}"

echo "WARNING: This will restore the database from backup."
echo "Database: ${DB_NAME}"
echo "Backup file: ${BACKUP_FILE}"
read -p "Are you sure? (yes/no): " CONFIRM

if [ "$CONFIRM" != "yes" ]; then
  echo "Restore cancelled."
  exit 0
fi

echo "Starting restore..."

# æ¢å¤æ•°æ®åº“
gunzip -c ${BACKUP_FILE} | PGPASSWORD="${DB_PASSWORD}" pg_restore \
  -h ${DB_HOST} \
  -p ${DB_PORT} \
  -U ${DB_USER} \
  -d ${DB_NAME} \
  -c \
  -v

echo "Restore completed successfully."
```

### MongoDBå¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# scripts/backup/mongo_backup.sh

MONGO_URI="${MONGO_URI:-mongodb://localhost:27017}"
DB_NAME="${DB_NAME:-payment_db}"
BACKUP_DIR="${BACKUP_DIR:-/backups/mongodb}"
DATE=$(date +%Y%m%d_%H%M%S)

mkdir -p ${BACKUP_DIR}

echo "Starting MongoDB backup..."

mongodump \
  --uri="${MONGO_URI}" \
  --db=${DB_NAME} \
  --out="${BACKUP_DIR}/${DATE}" \
  --gzip

# æ‰“åŒ…
tar -czf "${BACKUP_DIR}/${DB_NAME}_${DATE}.tar.gz" -C "${BACKUP_DIR}" "${DATE}"
rm -rf "${BACKUP_DIR}/${DATE}"

echo "Backup completed: ${BACKUP_DIR}/${DB_NAME}_${DATE}.tar.gz"
```

---

## ç´¢å¼•ä¼˜åŒ–ç­–ç•¥

### PostgreSQLç´¢å¼•ä¼˜åŒ–

```sql
-- ç´¢å¼•ä¼˜åŒ–åˆ†æè„šæœ¬
-- scripts/optimization/analyze_indexes.sql

-- 1. æŸ¥æ‰¾æœªä½¿ç”¨çš„ç´¢å¼•
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan as index_scans,
    pg_size_pretty(pg_relation_size(indexrelid)) as index_size
FROM pg_stat_user_indexes
WHERE idx_scan = 0
  AND indexrelname NOT LIKE 'pk_%'
  AND indexrelname NOT LIKE '%_pkey'
ORDER BY pg_relation_size(indexrelid) DESC;

-- 2. æŸ¥æ‰¾ç¼ºå¤±çš„ç´¢å¼•ï¼ˆåŸºäºæŸ¥è¯¢ç»Ÿè®¡ï¼‰
SELECT
    schemaname,
    tablename,
    seq_scan,
    seq_tup_read,
    idx_scan,
    idx_tup_fetch,
    CASE WHEN seq_scan > 0
        THEN round(100.0 * idx_scan / (seq_scan + idx_scan), 2)
        ELSE 100
    END as index_usage_percent
FROM pg_stat_user_tables
WHERE seq_scan > 100  -- å…¨è¡¨æ‰«æè¶…è¿‡100æ¬¡
  AND pg_relation_size(relid) > 10000000  -- è¡¨å¤§äº10MB
ORDER BY seq_scan DESC;

-- 3. ç´¢å¼•å¤§å°ç»Ÿè®¡
SELECT
    tablename,
    indexname,
    pg_size_pretty(pg_relation_size(indexrelid)) as index_size,
    pg_size_pretty(pg_relation_size(relid)) as table_size
FROM pg_stat_user_indexes
ORDER BY pg_relation_size(indexrelid) DESC
LIMIT 20;

-- 4. å»ºè®®æ·»åŠ çš„ç´¢å¼•
-- åŸºäºæŸ¥è¯¢æ¨¡å¼ï¼Œä»¥ä¸‹æ˜¯å»ºè®®çš„ç´¢å¼•ï¼š

-- è´¦æˆ·æŸ¥è¯¢ä¼˜åŒ–
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_bank_accounts_customer_id_status
ON bank_accounts(customer_id, status);

-- äº¤æ˜“æŸ¥è¯¢ä¼˜åŒ–
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_transactions_account_created
ON transactions(account_id, created_at DESC);

-- å¤åˆç´¢å¼•ä¼˜åŒ–ï¼ˆè¦†ç›–ç´¢å¼•ï¼‰
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_bank_accounts_covering
ON bank_accounts(account_number) INCLUDE (balance, status);
```

### MongoDBç´¢å¼•ä¼˜åŒ–

```javascript
// scripts/optimization/mongo_indexes.js

// 1. åˆ†ææ…¢æŸ¥è¯¢
db.setProfilingLevel(1, { slowms: 100 });

// 2. æŸ¥çœ‹ç´¢å¼•ä½¿ç”¨æƒ…å†µ
db.payments.aggregate([
  { $indexStats: {} }
]);

// 3. å»ºè®®çš„ç´¢å¼•
// æ”¯ä»˜è®¢å•æŸ¥è¯¢
db.payments.createIndex(
  { orderId: 1 },
  { unique: true, background: true }
);

// ç”¨æˆ·æ”¯ä»˜å†å²
db.payments.createIndex(
  { userId: 1, createdAt: -1 },
  { background: true }
);

// çŠ¶æ€æŸ¥è¯¢
db.payments.createIndex(
  { status: 1, createdAt: -1 },
  { background: true }
);

// å¤åˆç´¢å¼•
db.payments.createIndex(
  { userId: 1, status: 1, createdAt: -1 },
  { background: true }
);

// 4. åˆ é™¤æœªä½¿ç”¨çš„ç´¢å¼•
// db.payments.dropIndex("unused_index_name");
```

---

## æ•°æ®è„±æ•è§„åˆ™

### è„±æ•é…ç½®

```yaml
# config/data-masking.yml
masking_rules:
  # å®¢æˆ·è¡¨è„±æ•è§„åˆ™
  customers:
    - column: phone
      type: phone
      rule: "ä¿ç•™å‰3å4"
      example: "138****1234"
    - column: id_card
      type: id_card
      rule: "ä¿ç•™å‰4å4"
      example: "3201****5678"
    - column: email
      type: email
      rule: "ä¿ç•™é¦–å­—ç¬¦"
      example: "z****@example.com"
    - column: name
      type: name
      rule: "ä¿ç•™å§“"
      example: "å¼ **"
    - column: address
      type: address
      rule: "ä¿ç•™çœå¸‚"
      example: "æ±Ÿè‹çœå—äº¬å¸‚****"

  # è´¦æˆ·è¡¨è„±æ•è§„åˆ™
  bank_accounts:
    - column: account_number
      type: bank_card
      rule: "ä¿ç•™å‰6å4"
      example: "622848****1234"

  # äº¤æ˜“è¡¨è„±æ•è§„åˆ™
  transactions:
    - column: remark
      type: text
      rule: "å…¨éƒ¨è„±æ•"
      example: "****"
```

### è„±æ•è„šæœ¬

```python
# scripts/data_masking.py
import re
from typing import Callable, Dict

class DataMasker:
    """æ•°æ®è„±æ•å·¥å…·ç±»"""

    @staticmethod
    def mask_phone(phone: str) -> str:
        """æ‰‹æœºå·è„±æ•ï¼šä¿ç•™å‰3å4"""
        if not phone or len(phone) != 11:
            return phone
        return phone[:3] + '****' + phone[7:]

    @staticmethod
    def mask_id_card(id_card: str) -> str:
        """èº«ä»½è¯è„±æ•ï¼šä¿ç•™å‰4å4"""
        if not id_card or len(id_card) < 8:
            return id_card
        return id_card[:4] + '****' + id_card[-4:]

    @staticmethod
    def mask_bank_card(card: str) -> str:
        """é“¶è¡Œå¡è„±æ•ï¼šä¿ç•™å‰6å4"""
        if not card or len(card) < 10:
            return card
        return card[:6] + '****' + card[-4:]

    @staticmethod
    def mask_email(email: str) -> str:
        """é‚®ç®±è„±æ•ï¼šä¿ç•™é¦–å­—ç¬¦"""
        if not email or '@' not in email:
            return email
        at_index = email.index('@')
        return email[0] + '****' + email[at_index:]

    @staticmethod
    def mask_name(name: str) -> str:
        """å§“åè„±æ•ï¼šä¿ç•™å§“"""
        if not name or len(name) < 2:
            return name
        return name[0] + '**'

def mask_export_data(df, table_name: str, rules: Dict):
    """å¯¹å¯¼å‡ºæ•°æ®è¿›è¡Œè„±æ•å¤„ç†"""
    masker = DataMasker()
    table_rules = rules.get(table_name, [])

    for rule in table_rules:
        column = rule['column']
        mask_type = rule['type']

        if column in df.columns:
            mask_func = getattr(masker, f'mask_{mask_type}', None)
            if mask_func:
                df[column] = df[column].apply(mask_func)

    return df
```

---

## åˆ†åŒºç­–ç•¥

### PostgreSQLè¡¨åˆ†åŒº

```sql
-- äº¤æ˜“è¡¨æŒ‰æœˆåˆ†åŒº
-- scripts/partitioning/create_partitions.sql

-- åˆ›å»ºåˆ†åŒºä¸»è¡¨
CREATE TABLE transactions_partitioned (
    id BIGSERIAL,
    transaction_id VARCHAR(32) NOT NULL,
    account_id BIGINT NOT NULL,
    amount DECIMAL(19, 2) NOT NULL,
    transaction_type VARCHAR(20) NOT NULL,
    status VARCHAR(20) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL,
    updated_at TIMESTAMP WITH TIME ZONE,
    PRIMARY KEY (id, created_at)
) PARTITION BY RANGE (created_at);

-- åˆ›å»ºæœˆåº¦åˆ†åŒº
CREATE TABLE transactions_2026_01 PARTITION OF transactions_partitioned
    FOR VALUES FROM ('2026-01-01') TO ('2026-02-01');

CREATE TABLE transactions_2026_02 PARTITION OF transactions_partitioned
    FOR VALUES FROM ('2026-02-01') TO ('2026-03-01');

-- åˆ›å»ºåˆ†åŒºç´¢å¼•
CREATE INDEX idx_transactions_2026_01_account
ON transactions_2026_01(account_id, created_at DESC);

-- è‡ªåŠ¨åˆ›å»ºåˆ†åŒºçš„å‡½æ•°
CREATE OR REPLACE FUNCTION create_monthly_partition()
RETURNS void AS $$
DECLARE
    partition_date DATE;
    partition_name TEXT;
    start_date DATE;
    end_date DATE;
BEGIN
    partition_date := DATE_TRUNC('month', CURRENT_DATE + INTERVAL '1 month');
    partition_name := 'transactions_' || TO_CHAR(partition_date, 'YYYY_MM');
    start_date := partition_date;
    end_date := partition_date + INTERVAL '1 month';

    EXECUTE format(
        'CREATE TABLE IF NOT EXISTS %I PARTITION OF transactions_partitioned
         FOR VALUES FROM (%L) TO (%L)',
        partition_name, start_date, end_date
    );

    RAISE NOTICE 'Created partition: %', partition_name;
END;
$$ LANGUAGE plpgsql;

-- å®šæœŸè°ƒåº¦ï¼ˆä½¿ç”¨pg_cronï¼‰
-- SELECT cron.schedule('create-monthly-partition', '0 0 25 * *', 'SELECT create_monthly_partition()');
```

### MongoDBåˆ†ç‰‡é…ç½®

```javascript
// scripts/partitioning/mongo_sharding.js

// å¯ç”¨åˆ†ç‰‡
sh.enableSharding("payment_db");

// å¯¹paymentsé›†åˆæŒ‰æ—¶é—´åˆ†ç‰‡
sh.shardCollection(
  "payment_db.payments",
  { createdAt: 1, paymentId: 1 }  // å¤åˆåˆ†ç‰‡é”®
);

// é…ç½®åˆ†ç‰‡åŒºåŸŸï¼ˆå¯é€‰ï¼‰
sh.addShardTag("shard0001", "hot");
sh.addShardTag("shard0002", "warm");
sh.addShardTag("shard0003", "cold");

// æŒ‰æ—¶é—´èŒƒå›´åˆ†é…åˆ°ä¸åŒåˆ†ç‰‡
sh.addTagRange(
  "payment_db.payments",
  { createdAt: new Date("2026-01-01") },
  { createdAt: new Date("2026-02-01") },
  "hot"
);
```

---

## æ•°æ®è´¨é‡æ£€æŸ¥

### æ•°æ®è´¨é‡è§„åˆ™

```python
# scripts/data_quality/quality_checks.py
from dataclasses import dataclass
from typing import List, Dict, Any
import pandas as pd

@dataclass
class QualityRule:
    name: str
    table: str
    column: str
    rule_type: str
    threshold: float
    description: str

class DataQualityChecker:
    def __init__(self, connection):
        self.conn = connection
        self.rules = self._load_rules()

    def _load_rules(self) -> List[QualityRule]:
        return [
            QualityRule("completeness_account_number", "bank_accounts", "account_number", "not_null", 1.0, "è´¦å·ä¸èƒ½ä¸ºç©º"),
            QualityRule("completeness_balance", "bank_accounts", "balance", "not_null", 1.0, "ä½™é¢ä¸èƒ½ä¸ºç©º"),
            QualityRule("uniqueness_account_number", "bank_accounts", "account_number", "unique", 1.0, "è´¦å·å¿…é¡»å”¯ä¸€"),
            QualityRule("range_balance", "bank_accounts", "balance", "range", 0.0, "ä½™é¢ä¸èƒ½ä¸ºè´Ÿ"),
            QualityRule("format_phone", "customers", "phone", "regex", 0.99, "æ‰‹æœºå·æ ¼å¼"),
        ]

    def check_not_null(self, table: str, column: str) -> Dict:
        query = f"SELECT COUNT(*) as total, COUNT({column}) as non_null FROM {table}"
        df = pd.read_sql(query, self.conn)
        rate = df['non_null'].iloc[0] / df['total'].iloc[0]
        return {"rate": rate, "passed": rate >= 1.0}

    def check_unique(self, table: str, column: str) -> Dict:
        query = f"""
            SELECT COUNT(*) as total,
                   COUNT(DISTINCT {column}) as distinct_count
            FROM {table}
        """
        df = pd.read_sql(query, self.conn)
        rate = df['distinct_count'].iloc[0] / df['total'].iloc[0]
        return {"rate": rate, "passed": rate >= 1.0}

    def run_all_checks(self) -> List[Dict]:
        results = []
        for rule in self.rules:
            if rule.rule_type == "not_null":
                result = self.check_not_null(rule.table, rule.column)
            elif rule.rule_type == "unique":
                result = self.check_unique(rule.table, rule.column)
            else:
                result = {"rate": 1.0, "passed": True}

            results.append({
                "rule": rule.name,
                "table": rule.table,
                "column": rule.column,
                **result
            })
        return results

    def generate_report(self) -> Dict:
        results = self.run_all_checks()
        passed = sum(1 for r in results if r['passed'])
        return {
            "total_rules": len(results),
            "passed": passed,
            "failed": len(results) - passed,
            "score": round(passed / len(results) * 100, 2),
            "details": results
        }
```

---

## ç›¸å…³èµ„æº

- Agentå¯åŠ¨æç¤ºè¯: `agent_prompts.md`
- è¯¦ç»†å·¥ä½œè®¡åˆ’: `digital_bank_poc_workplan.md`
- **æŠ€æœ¯æ ‡å‡†è§„èŒƒ v1.0**: `docs/architecture/technical-standards-v1.0.md` âš ï¸ **å¿…é¡»éµå¾ª**
- **å‘½åè§„èŒƒ v1.0**: `docs/architecture/naming-conventions.md` âš ï¸ **å¿…é¡»éµå¾ª**

---

**ç‰ˆæœ¬**: v1.1.0  
**åˆ›å»ºæ—¥æœŸ**: 2026-01-26  
**ç»´æŠ¤è€…**: Digital Bank POC Team
